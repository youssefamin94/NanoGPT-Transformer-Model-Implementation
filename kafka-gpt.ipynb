{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8390969,"sourceType":"datasetVersion","datasetId":4991154}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\n# hyperparameters\nbatch_size = 64\nblock_size = 256\nmax_iters = 4000\neval_interval = 100\nlearning_rate = 0.0003\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 512  # Increased embedding dimension\nn_head = 6\nn_layer = 6\ndropout = .2\n\ntorch.manual_seed(1337)\n\nwith open('/kaggle/input/kafkaswork/Kafka.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nclass BPE_Tokenizer:\n    def __init__(self, vocab):\n        self.vocab = vocab\n        self.stoi = {ch: i for i, ch in enumerate(self.vocab)}\n        self.itos = {i: ch for i, ch in enumerate(self.vocab)}\n\n    def encode(self, s):\n        tokens = list(s)\n        return [self.stoi[t] for t in tokens]\n\n    def decode(self, l):\n        return ''.join([self.itos[i] for i in l])\n\ntokenizer = BPE_Tokenizer(chars)\nencode = tokenizer.encode\ndecode = tokenizer.decode\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(num_heads * head_size, n_embd)  # Adjusted here\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=3000)[0].tolist()))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-12T12:22:34.050729Z","iopub.execute_input":"2024-05-12T12:22:34.051131Z","iopub.status.idle":"2024-05-12T13:26:03.370643Z","shell.execute_reply.started":"2024-05-12T12:22:34.051103Z","shell.execute_reply":"2024-05-12T13:26:03.369740Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"19.096658 M parameters\nstep 0: train loss 4.6322, val loss 4.6278\nstep 100: train loss 2.4216, val loss 2.4325\nstep 200: train loss 2.3674, val loss 2.3799\nstep 300: train loss 2.2881, val loss 2.3082\nstep 400: train loss 2.0991, val loss 2.1299\nstep 500: train loss 1.9020, val loss 1.9451\nstep 600: train loss 1.7551, val loss 1.8090\nstep 700: train loss 1.6544, val loss 1.7234\nstep 800: train loss 1.5714, val loss 1.6461\nstep 900: train loss 1.5072, val loss 1.5922\nstep 1000: train loss 1.4548, val loss 1.5437\nstep 1100: train loss 1.4123, val loss 1.5109\nstep 1200: train loss 1.3739, val loss 1.4780\nstep 1300: train loss 1.3398, val loss 1.4562\nstep 1400: train loss 1.3089, val loss 1.4299\nstep 1500: train loss 1.2874, val loss 1.4225\nstep 1600: train loss 1.2602, val loss 1.3972\nstep 1700: train loss 1.2401, val loss 1.3835\nstep 1800: train loss 1.2247, val loss 1.3783\nstep 1900: train loss 1.2085, val loss 1.3691\nstep 2000: train loss 1.1880, val loss 1.3542\nstep 2100: train loss 1.1736, val loss 1.3486\nstep 2200: train loss 1.1588, val loss 1.3451\nstep 2300: train loss 1.1418, val loss 1.3380\nstep 2400: train loss 1.1287, val loss 1.3323\nstep 2500: train loss 1.1148, val loss 1.3240\nstep 2600: train loss 1.1024, val loss 1.3225\nstep 2700: train loss 1.0915, val loss 1.3233\nstep 2800: train loss 1.0789, val loss 1.3173\nstep 2900: train loss 1.0636, val loss 1.3209\nstep 3000: train loss 1.0552, val loss 1.3147\nstep 3100: train loss 1.0427, val loss 1.3219\nstep 3200: train loss 1.0345, val loss 1.3203\nstep 3300: train loss 1.0171, val loss 1.3157\nstep 3400: train loss 1.0088, val loss 1.3143\nstep 3500: train loss 0.9959, val loss 1.3195\nstep 3600: train loss 0.9862, val loss 1.3162\nstep 3700: train loss 0.9730, val loss 1.3259\nstep 3800: train loss 0.9627, val loss 1.3232\nstep 3900: train loss 0.9515, val loss 1.3302\nstep 3999: train loss 0.9405, val loss 1.3378\n\n\" 'Thus given me. I must have come to how could I ran out a far. But ah, my misery \nnod, also be quietly over my burrow, I have the same time thought over the distraction \nof the real of the top of his runs, but for it would be until think. That makes \none haid better for our piping and chosts following there.\" He asked a hunting man \ninterder and so discover for our tonume. Among this clouds, there, powers about one \nset, all passeng on these. The engagements; and totations may be two off themselves to take that \nhim as if all its thought by the best place of these words of their struggles honests in matues, \noccurred to anieu. \nScociently when Blumfeld used to be an around; dur this fulfed not \nguards into a perpetual scientifying but for science, the sakes is under instance, \nand too, it may be easient. When he is tautbing now our oon many with people wanderful carriages. There is a \nbannouncement else. \nEverything elseess else specially in depensable comparence against that happens higher being \nannounced. Only could I have learned that something comel; the fact the man has \narrived to get ettract assembly the officer is food. \nAnd now I am in anyone or a moment as faster that now, and I dear near any ray ei in the \nstart. I tho explorer decree,\" temption withd, and a statisfaction. Then when the assembly \ncommentant langest on aparently suvarios anything. Should I feel comfort, it is at least \nresearches. How perhaps surely I am joy in is this devotion adminishment, I \nburrow simply have been carrying over me and besides it. And something else, as a \nsman except.\" \nWhen I do gazing home of any time he already due the wine, jailed as I \nwant to slaw in ordinary movement. Instead than by this meason of my could \nuse to be avantage. Which is the apes back from my quet you would gives all come of likely, \ntoday. \nWhen, people aleve dogs who is possible for thinking of a Constmath.' Come anbore to \nget me with me struggers, and then I ement cause. \nOnly it never do I do ask for it better, as I feel as care it and I were conferred in spite. \nWhen I did this mouth eternal respectfully and I might tell you \ngo to some out: I shoul haven't important you. Out with what \nfear fruit, but gives it? That most pure only left.  \nCome all I have doness if some real complete world. I can ask my experience have been \nsuch an art of hand. You may be observed by middle similar, is of forced you won't \nrepresent me than this fact why goods? Of course, who's exactly all have exacted enough. \nNo, whenever you have actual obstinate rise for making; you see no mead a draw \nshow istreak in your firm town on your further all reeling he posters, but I've even have \npostered since you anyone would beaut conceal forest of those maddarity years' design; then \nyou lose the chiefs that this eyer since on their suitcase was broad by the soulder \nnecks of mountains charated by my experiments of their good joins. I always were \ntrouble in or devoluting mpose persone of the vibr\n","output_type":"stream"}]},{"cell_type":"code","source":"print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T13:26:49.721361Z","iopub.execute_input":"2024-05-12T13:26:49.721749Z","iopub.status.idle":"2024-05-12T13:27:06.732235Z","shell.execute_reply.started":"2024-05-12T13:26:49.721721Z","shell.execute_reply":"2024-05-12T13:27:06.731296Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\nbroughly before. Mrs!\" Hear -- \"No to say, yelGilled has jumps up onto the hall weep.\" \nHis right accompanied. Why as fasting for Georg, the assistant \nwhen he was sitting his visit, stakes a louder, seanted back as and un a stranger weigh \ntakes and among arouble two quite well-mescaper despair, that was in his speechness and \nachieved his legs. In this start lace, the beast scord before small schools that \nkind of have and surely bettered -- ahd as must fall fares in a cecombar sense sduce of \nburdeness him in the idea, and half-o a fult of races jump, he is it not very funting down \nwhen hims might over the help of a time to himself, for in any condition of the \nheart laugh, but often burrow off bearers course to him after a flee will be dark \nbudge that itself up tolerate work. \nTranials him down K. \nSee aloud with a tlightful walk and his rise after toward me when I like by the \nsister was nearly too valid ago, after that this they are now worry how such a \nvery gister penety, or \n","output_type":"stream"}]},{"cell_type":"code","source":"print(decode(m.generate(context, max_new_tokens=900)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T13:29:04.313586Z","iopub.execute_input":"2024-05-12T13:29:04.314110Z","iopub.status.idle":"2024-05-12T13:29:20.020217Z","shell.execute_reply.started":"2024-05-12T13:29:04.314075Z","shell.execute_reply":"2024-05-12T13:29:20.019307Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\nnow it wasn't a character. Even strange pavements, contemplation on Josephine's binds, \nhaven't inclined but Josephine, must have had always been possible thing explicitly, \nwhich inside it was something or assistance is micdle as an evertaken that. Our occasions \nstand tolerate to understands another the assistants of the onlookers. Quarge, and their \nexistent meant to keep them and fear jumped too; for a long day dog \nnothing else; not during earlier one of the moletation horses seen might neces for me \nandn't out. \nA few mining, which I live now admit is, were pretending in course. They all comfortate \nsomewhere else eff and the last time again when that evening I found myself had \nto distroge and difficult, flung themselves and fell covered by the such hanging me \nunderstanding, staring which aparently enjoyed him to appear the basing act scase, \noften why he those planess \nwould not\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}